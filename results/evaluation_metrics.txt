Impact of Imputation Methods on the Distribution Characteristics of the 'GarageYrBlt' Feature

| Statistic  | No Imputation | -1 Imputation | 0 Imputation |
|------------|---------------|---------------|--------------|
| Count      | 1460          | 1460          | 1460         |
| Mean       | 1868.74       | 1868.68       | 1868.74      |
| Std Dev    | 453.70        | 453.93        | 453.70       |
| Min        | 0.00          | -1.00         | 0.00         |
| 25th %ile  | 1958.00       | 1958.00       | 1958.00      |
| Median     | 1977.00       | 1977.00       | 1977.00      |
| 75th %ile  | 2001.00       | 2001.00       | 2001.00      |
| Max        | 2010.00       | 2010.00       | 2010.00      |

Spearman Correlation Analysis

This section presents the Spearman correlation coefficients and p-values between various features and the target variable, `SalePrice`. Spearman correlation is used here to assess the monotonic relationships between variables.

| Feature       | Corr_coef | P_values             |
|---------------|-----------|----------------------|
| MSSubClass    | 0.583130  | 9.583345e-134        |
| MSZoning      | 0.422232  | 3.364273e-64         |
| Street        | 0.045814  | 8.012225e-02         |
| Alley         | 0.130207  | 5.968981e-07         |
| LotShape      | 0.321184  | 2.188686e-36         |
| LandContour   | 0.154423  | 3.010003e-09         |
| Utilities     | 0.016710  | 5.234926e-01         |
| LotConfig     | 0.105029  | 5.798914e-05         |
| LandSlope     | 0.050310  | 5.461545e-02         |
| Neighborhood  | 0.755779  | 2.258361e-270        |
| Condition1    | 0.202877  | 5.003681e-15         |
| Condition2    | 0.107173  | 4.071644e-05         |
| BldgType      | 0.151785  | 5.592082e-09         |
| HouseStyle    | 0.340038  | 7.654791e-41         |
| RoofStyle     | 0.163782  | 3.068415e-10         |
| RoofMatl      | 0.090497  | 5.361225e-04         |
| Exterior1st   | 0.435284  | 1.485049e-68         |
| Exterior2nd   | 0.423422  | 1.373259e-64         |
| MasVnrType    | 0.420367  | 1.359797e-63         |
| Foundation    | 0.573580  | 1.755079e-128        |
| BsmtFinType1  | 0.454237  | 3.151705e-75         |
| BsmtFinType2  | 0.158796  | 1.053183e-09         |
| Heating       | 0.121949  | 2.966772e-06         |
| CentralAir    | 0.313286  | 1.302833e-34         |
| Electrical    | 0.298652  | 1.835158e-31         |
| Functional    | 0.136477  | 1.651288e-07         |
| GarageType    | 0.598814  | 8.906579e-143        |
| GarageFinish  | 0.633974  | 5.937952e-165        |
| GarageQual    | 0.351110  | 1.314662e-43         |
| GarageCond    | 0.351653  | 9.555301e-44         |
| PavedDrive    | 0.280602  | 8.018188e-28         |
| Fence         | 0.209028  | 7.063457e-16         |
| MiscFeature   | 0.078318  | 2.748476e-03         |
| SaleType      | 0.307845  | 2.026248e-33         |
| SaleCondition | 0.320496  | 3.140674e-36         |


Note-: Inverse RMSE corresponds to RMSE calculated in dollars with true value while RMSE corrsponds to metric evaluated on normalizaed target variable.

Baseline Model Performance Metrics
......................................

Linear Regression:

Mean Squared Error (MSE): 0.017209
Root Mean Squared Error (RMSE): 0.131182
Mean Absolute Error (MAE): 0.089815
Inverse MSE: 5.363059e+08
RMSE (in $): 23158.28
MAE (in $): 15061.21
R-squared (R²): 0.930080
Linear Regression serves as the baseline model, providing a reference point for comparing more complex models. Its performance metrics indicate a strong fit with a high R² score.

Random Forest Regressor:

Mean Squared Error (MSE): 0.021490
Root Mean Squared Error (RMSE): 0.146596
Mean Absolute Error (MAE): 0.099369
Inverse MSE: 8.289417e+08
RMSE (in $): 28791.35
MAE (in $): 17469.71
R-squared (R²): 0.891929
Random Forest Regressor offers better handling of non-linear relationships and interactions between features, with slightly lower R² compared to Linear Regression.

XGBoost Regressor:

Mean Squared Error (MSE): 0.021017
Root Mean Squared Error (RMSE): 0.144971
Mean Absolute Error (MAE): 0.099638
Inverse MSE: 6.881003e+08
RMSE (in $): 26231.67
MAE (in $): 17367.99
R-squared (R²): 0.910291
XGBoost Regressor, known for its robustness and flexibility, demonstrates competitive performance with a high R² score, indicating its efficacy in capturing complex patterns.

MLP Regressor:

Mean Squared Error (MSE): 0.029048
Root Mean Squared Error (RMSE): 0.170435
Mean Absolute Error (MAE): 0.124869
Inverse MSE: 1.257921e+09
RMSE (in $): 35467.19
MAE (in $): 22592.49
R-squared (R²): 0.836002
MLP Regressor, a neural network-based approach, has the highest error metrics and the lowest R² score among the models, indicating room for improvement in capturing the underlying data patterns.

Hyperparameter Tuned Models
....................................

Random Forest Regressor (Tuned):

Mean Squared Error (MSE): 0.021863
Root Mean Squared Error (RMSE): 0.147863
Mean Absolute Error (MAE): 0.097896
Inverse MSE: 8.916001e+08
RMSE (in $): 29859.67
MAE (in $): 17373.43
R-squared (R²): 0.883760
Tuning improves the Random Forest Regressor’s performance slightly, yet it still falls short of the baseline Linear Regression in terms of R².

XGBoost Regressor (Tuned):

Mean Squared Error (MSE): 0.019765
Root Mean Squared Error (RMSE): 0.140589
Mean Absolute Error (MAE): 0.093013
Inverse MSE: 8.080508e+08
RMSE (in $): 28426.23
MAE (in $): 16432.65
R-squared (R²): 0.894652
Tuning enhances the XGBoost Regressor, improving its performance metrics, and brings its R² closer to that of the baseline Linear Regression.

MLP Regressor (Tuned):

Mean Squared Error (MSE): 0.016535
Root Mean Squared Error (RMSE): 0.128587
Mean Absolute Error (MAE): 0.091750
Inverse MSE: 5.293362e+08
RMSE (in $): 23007.31
MAE (in $): 15549.03
R-squared (R²): 0.930989
Tuning significantly improves the MLP Regressor’s performance, achieving metrics comparable to or better than the baseline model, with a high R² score.

Feature Engineered Dataset-Based Model Performance Metrics
..................................................................

Random Forest Regressor:

Mean Squared Error (MSE): 0.021933
Root Mean Squared Error (RMSE): 0.148097
Mean Absolute Error (MAE): 0.098270
Inverse MSE: 9.257473e+08
RMSE (in $): 30426.10
MAE (in $): 17522.78
R-squared (R²): 0.879308
The addition of engineered features leads to a slight increase in error metrics for the Random Forest Regressor, reflecting the impact of feature changes.

XGBoost Regressor:

Mean Squared Error (MSE): 0.021970
Root Mean Squared Error (RMSE): 0.148224
Mean Absolute Error (MAE): 0.098200
Inverse MSE: 7.659826e+08
RMSE (in $): 27676.39
MAE (in $): 17223.00
R-squared (R²): 0.900137
Feature engineering slightly degrades the XGBoost Regressor’s performance, though it remains strong compared to other models.

MLP Regressor:

Mean Squared Error (MSE): 0.025420
Root Mean Squared Error (RMSE): 0.159436
Mean Absolute Error (MAE): 0.118454
Inverse MSE: 9.183608e+08
RMSE (in $): 30304.47
MAE (in $): 20811.16
R-squared (R²): 0.880271
The performance of the MLP Regressor deteriorates with feature engineering, with the highest error metrics among the feature-engineered models.

Voting Regressor Model Performance with Hard Voting Method Using (RF, XGB, and MLP)
.............................................................................

Cross-validation Score for Voting Regressor Model: 0.140947
Mean Squared Error (MSE):
Root Mean Squared Error (RMSE): 0.124772
Root Mean Squared Error (RMSE) (in $): 23136.85
Mean Absolute Error (MAE) (in $): 14196.87
The Voting Regressor, combining RF, XGB, and MLP models, performs well with a robust RMSE and MAE, providing a balanced prediction.

Stacking Regressor Model Performance with RF as the Meta Model (Hyperparameter Tuned)
......................................................................................

Root Mean Squared Error (RMSE): 0.137281
Root Mean Squared Error (RMSE) (in $): 32355.92
Mean Absolute Error (MAE) (in $): 16893.47
The Stacking Regressor model with RF as the meta-model shows higher RMSE and MAE compared to other models, indicating potential areas for further tuning or feature adjustment.
