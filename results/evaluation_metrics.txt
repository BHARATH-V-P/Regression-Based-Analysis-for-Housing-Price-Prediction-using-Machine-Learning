Baseline Model Performance Metrics
......................................

Linear Regression:

Mean Squared Error (MSE): 0.017209
Root Mean Squared Error (RMSE): 0.131182
Mean Absolute Error (MAE): 0.089815
Inverse MSE: 5.363059e+08
RMSE (in $): 23158.28
MAE (in $): 15061.21
R-squared (R²): 0.930080
Linear Regression serves as the baseline model, providing a reference point for comparing more complex models. Its performance metrics indicate a strong fit with a high R² score.

Random Forest Regressor:

Mean Squared Error (MSE): 0.021490
Root Mean Squared Error (RMSE): 0.146596
Mean Absolute Error (MAE): 0.099369
Inverse MSE: 8.289417e+08
RMSE (in $): 28791.35
MAE (in $): 17469.71
R-squared (R²): 0.891929
Random Forest Regressor offers better handling of non-linear relationships and interactions between features, with slightly lower R² compared to Linear Regression.

XGBoost Regressor:

Mean Squared Error (MSE): 0.021017
Root Mean Squared Error (RMSE): 0.144971
Mean Absolute Error (MAE): 0.099638
Inverse MSE: 6.881003e+08
RMSE (in $): 26231.67
MAE (in $): 17367.99
R-squared (R²): 0.910291
XGBoost Regressor, known for its robustness and flexibility, demonstrates competitive performance with a high R² score, indicating its efficacy in capturing complex patterns.

MLP Regressor:

Mean Squared Error (MSE): 0.029048
Root Mean Squared Error (RMSE): 0.170435
Mean Absolute Error (MAE): 0.124869
Inverse MSE: 1.257921e+09
RMSE (in $): 35467.19
MAE (in $): 22592.49
R-squared (R²): 0.836002
MLP Regressor, a neural network-based approach, has the highest error metrics and the lowest R² score among the models, indicating room for improvement in capturing the underlying data patterns.

Hyperparameter Tuned Models
....................................

Random Forest Regressor (Tuned):

Mean Squared Error (MSE): 0.021863
Root Mean Squared Error (RMSE): 0.147863
Mean Absolute Error (MAE): 0.097896
Inverse MSE: 8.916001e+08
RMSE (in $): 29859.67
MAE (in $): 17373.43
R-squared (R²): 0.883760
Tuning improves the Random Forest Regressor’s performance slightly, yet it still falls short of the baseline Linear Regression in terms of R².

XGBoost Regressor (Tuned):

Mean Squared Error (MSE): 0.019765
Root Mean Squared Error (RMSE): 0.140589
Mean Absolute Error (MAE): 0.093013
Inverse MSE: 8.080508e+08
RMSE (in $): 28426.23
MAE (in $): 16432.65
R-squared (R²): 0.894652
Tuning enhances the XGBoost Regressor, improving its performance metrics, and brings its R² closer to that of the baseline Linear Regression.

MLP Regressor (Tuned):

Mean Squared Error (MSE): 0.016535
Root Mean Squared Error (RMSE): 0.128587
Mean Absolute Error (MAE): 0.091750
Inverse MSE: 5.293362e+08
RMSE (in $): 23007.31
MAE (in $): 15549.03
R-squared (R²): 0.930989
Tuning significantly improves the MLP Regressor’s performance, achieving metrics comparable to or better than the baseline model, with a high R² score.

Feature Engineered Dataset-Based Model Performance Metrics
..................................................................

Random Forest Regressor:

Mean Squared Error (MSE): 0.021933
Root Mean Squared Error (RMSE): 0.148097
Mean Absolute Error (MAE): 0.098270
Inverse MSE: 9.257473e+08
RMSE (in $): 30426.10
MAE (in $): 17522.78
R-squared (R²): 0.879308
The addition of engineered features leads to a slight increase in error metrics for the Random Forest Regressor, reflecting the impact of feature changes.

XGBoost Regressor:

Mean Squared Error (MSE): 0.021970
Root Mean Squared Error (RMSE): 0.148224
Mean Absolute Error (MAE): 0.098200
Inverse MSE: 7.659826e+08
RMSE (in $): 27676.39
MAE (in $): 17223.00
R-squared (R²): 0.900137
Feature engineering slightly degrades the XGBoost Regressor’s performance, though it remains strong compared to other models.

MLP Regressor:

Mean Squared Error (MSE): 0.025420
Root Mean Squared Error (RMSE): 0.159436
Mean Absolute Error (MAE): 0.118454
Inverse MSE: 9.183608e+08
RMSE (in $): 30304.47
MAE (in $): 20811.16
R-squared (R²): 0.880271
The performance of the MLP Regressor deteriorates with feature engineering, with the highest error metrics among the feature-engineered models.

Voting Regressor Model Performance with Hard Voting Method Using (RF, XGB, and MLP)
.............................................................................

Cross-validation Score for Voting Regressor Model: 0.140947
Mean Squared Error (MSE):
Root Mean Squared Error (RMSE): 0.124772
Root Mean Squared Error (RMSE) (in $): 23136.85
Mean Absolute Error (MAE) (in $): 14196.87
The Voting Regressor, combining RF, XGB, and MLP models, performs well with a robust RMSE and MAE, providing a balanced prediction.

Stacking Regressor Model Performance with RF as the Meta Model (Hyperparameter Tuned)
......................................................................................

Root Mean Squared Error (RMSE): 0.137281
Root Mean Squared Error (RMSE) (in $): 32355.92
Mean Absolute Error (MAE) (in $): 16893.47
The Stacking Regressor model with RF as the meta-model shows higher RMSE and MAE compared to other models, indicating potential areas for further tuning or feature adjustment.
